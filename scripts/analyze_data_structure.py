#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–ø–∫–µ data/train.
–°–æ–±–∏—Ä–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ:
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ –æ–±—ä–µ–∫—Ç–æ–≤ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π —Å Li_–∫–∞—Ä—Ç—ã –∏ Ae_–Ω–µ–º–µ—Ü–∫–∞—è –¥–∞–Ω–Ω—ã–º–∏
"""

import os
import json
from pathlib import Path
from collections import defaultdict, Counter
import re


def analyze_data_structure(data_path):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–ø–∫–µ data/train

    Args:
        data_path (str): –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ data/train

    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
    """
    data_path = Path(data_path)

    if not data_path.exists():
        raise ValueError(f"–ü—É—Ç—å {data_path} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

    results = {
        "total_territories": 0,
        "territories_with_li": 0,
        "territories_with_ae": 0,
        "class_counts": Counter(),
        "territory_details": [],
        "data_type_counts": Counter(),
    }

    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ –∏–∑ –∏–º–µ–Ω–∏ –ø–∞–ø–∫–∏
    territory_pattern = re.compile(r"^\d+_(.+)_FINAL$")

    for territory_dir in sorted(data_path.iterdir()):
        if not territory_dir.is_dir():
            continue

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ø–∞–ø–∫–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ (—Å–æ–¥–µ—Ä–∂–∏—Ç _FINAL)
        territory_match = territory_pattern.match(territory_dir.name)
        if not territory_match:
            continue

        territory_name = territory_match.group(1)
        results["total_territories"] += 1

        territory_info = {
            "name": territory_name,
            "full_name": territory_dir.name,
            "has_li": False,
            "has_ae": False,
            "classes": set(),
            "data_types": set(),
        }

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–∞–ø–∫–∏ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏
        for subdir in territory_dir.iterdir():
            if not subdir.is_dir():
                continue

            subdir_name = subdir.name

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ Li –∫–∞—Ä—Ç
            if "Li_–∫–∞—Ä—Ç—ã" in subdir_name or "LI_–∫–∞—Ä—Ç—ã" in subdir_name:
                territory_info["has_li"] = True
                results["territories_with_li"] += 1
                territory_info["data_types"].add("Li")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ Ae –Ω–µ–º–µ—Ü–∫–∏—Ö –∫–∞—Ä—Ç
            elif "Ae_–Ω–µ–º–µ—Ü–∫–∞—è" in subdir_name or "AE_–Ω–µ–º–µ—Ü–∫–∞—è" in subdir_name:
                territory_info["has_ae"] = True
                results["territories_with_ae"] += 1
                territory_info["data_types"].add("Ae")

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞–ø–∫—É —Ä–∞–∑–º–µ—Ç–∫–∏
            elif "—Ä–∞–∑–º–µ—Ç–∫–∞" in subdir_name:
                # –ò—â–µ–º –ø–æ–¥–ø–∞–ø–∫–∏ Li –∏ Ae –≤ –ø–∞–ø–∫–µ —Ä–∞–∑–º–µ—Ç–∫–∏
                for markup_subdir in subdir.iterdir():
                    if not markup_subdir.is_dir():
                        continue

                    markup_subdir_name = markup_subdir.name

                    if markup_subdir_name in ["Li", "LI"]:
                        territory_info["data_types"].add("Li")
                        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ Li
                        for geojson_file in markup_subdir.glob("*.geojson"):
                            class_name = extract_class_from_filename(geojson_file.name, territory_name, "Li")
                            if class_name:
                                territory_info["classes"].add(class_name)
                                results["class_counts"][class_name] += 1

                    elif markup_subdir_name in ["Ae", "AE"]:
                        territory_info["data_types"].add("Ae")
                        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ Ae
                        for geojson_file in markup_subdir.glob("*.geojson"):
                            class_name = extract_class_from_filename(geojson_file.name, territory_name, "Ae")
                            if class_name:
                                territory_info["classes"].add(class_name)
                                results["class_counts"][class_name] += 1

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏
        for data_type in territory_info["data_types"]:
            results["data_type_counts"][data_type] += 1

        results["territory_details"].append(territory_info)

    return results


def extract_class_from_filename(filename, territory_name, data_type):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ geojson

    Args:
        filename (str): –ò–º—è —Ñ–∞–π–ª–∞
        territory_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏
        data_type (str): –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö (Li –∏–ª–∏ Ae)

    Returns:
        str: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞ –∏–ª–∏ None
    """
    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∞: {–¢–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è}_{–¢–∏–ø}_{–ö–ª–∞—Å—Å}.geojson
    pattern = f"{territory_name}_{data_type}_(.+)\\.geojson"
    match = re.search(pattern, filename, re.IGNORECASE)

    if match:
        return match.group(1)
    return None


def print_analysis_results(results):
    """
    –í—ã–≤–æ–¥–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ —É–¥–æ–±–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
    """
    print("=" * 80)
    print("–ê–ù–ê–õ–ò–ó –°–¢–†–£–ö–¢–£–†–´ –î–ê–ù–ù–´–•")
    print("=" * 80)

    print(f"\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   –í—Å–µ–≥–æ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π: {results['total_territories']}")
    print(f"   –¢–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π —Å Li –∫–∞—Ä—Ç–∞–º–∏: {results['territories_with_li']}")
    print(f"   –¢–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π —Å Ae –Ω–µ–º–µ—Ü–∫–∏–º–∏ –∫–∞—Ä—Ç–∞–º–∏: {results['territories_with_ae']}")

    print(f"\nüó∫Ô∏è –¢–ò–ü–´ –ò–°–•–û–î–ù–´–• –î–ê–ù–ù–´–•:")
    for data_type, count in results["data_type_counts"].most_common():
        print(f"   {data_type}: {count} —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π")

    print(f"\nüèõÔ∏è –ö–õ–ê–°–°–´ –û–ë–™–ï–ö–¢–û–í (–≤—Å–µ–≥–æ {len(results['class_counts'])} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤):")
    for class_name, count in results["class_counts"].most_common():
        print(f"   {class_name}: {count} —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π")

    print(f"\nüìç –î–ï–¢–ê–õ–¨–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –ü–û –¢–ï–†–†–ò–¢–û–†–ò–Ø–ú:")
    for territory in results["territory_details"]:
        data_types_str = ", ".join(sorted(territory["data_types"])) if territory["data_types"] else "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
        classes_str = ", ".join(sorted(territory["classes"])) if territory["classes"] else "–ù–µ—Ç –∫–ª–∞—Å—Å–æ–≤"
        print(f"   {territory['name']}:")
        print(f"     - –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö: {data_types_str}")
        print(f"     - –ö–ª–∞—Å—Å—ã –æ–±—ä–µ–∫—Ç–æ–≤: {classes_str}")


def save_results_to_json(results, output_path):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ JSON —Ñ–∞–π–ª
    """
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º Counter –æ–±—ä–µ–∫—Ç—ã –≤ –æ–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è JSON
    json_results = {
        "total_territories": results["total_territories"],
        "territories_with_li": results["territories_with_li"],
        "territories_with_ae": results["territories_with_ae"],
        "class_counts": dict(results["class_counts"]),
        "data_type_counts": dict(results["data_type_counts"]),
        "territory_details": [],
    }

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º set –æ–±—ä–µ–∫—Ç—ã –≤ —Å–ø–∏—Å–∫–∏ –¥–ª—è JSON
    for territory in results["territory_details"]:
        territory_json = {
            "name": territory["name"],
            "full_name": territory["full_name"],
            "has_li": territory["has_li"],
            "has_ae": territory["has_ae"],
            "classes": list(territory["classes"]),
            "data_types": list(territory["data_types"]),
        }
        json_results["territory_details"].append(territory_json)

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(json_results, f, ensure_ascii=False, indent=2)

    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}")


def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞
    """
    # –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–∞–Ω–Ω—ã–º–∏
    data_path = Path(__file__).parent.parent / "data" / "train"

    if not data_path.exists():
        print(f"‚ùå –û—à–∏–±–∫–∞: –ü–∞–ø–∫–∞ {data_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∏–∑ –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞.")
        return

    try:
        print("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö...")
        results = analyze_data_structure(data_path)

        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print_analysis_results(results)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON
        output_path = Path(__file__).parent / "data_analysis_results.json"
        save_results_to_json(results, output_path)

        print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}")
        raise


if __name__ == "__main__":
    main()
